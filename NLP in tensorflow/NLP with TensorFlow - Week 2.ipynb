{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n",
      "If tensorflow version is 1.x, use tf.enable_eager_execution()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"If tensorflow version is 1.x, use tf.enable_eager_execution()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Dataset Type1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\"> IMDB Dataset </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imdb reviews loaded from tensorflow datasets are not in the usable format. To convert them into rather simple and more easily readable formats, we have initialised empty lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now move the contents from train_data and test_data into the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentences, labels in train_data:\n",
    "    training_sentences.append(str(sentences.numpy()))\n",
    "    training_labels.append(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentences, labels in test_data:\n",
    "    testing_sentences.append(str(sentences.numpy()))\n",
    "    testing_labels.append(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Numpy arrays for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use this cell to introduce the hyperparameters. This way it would be easier to change them.\n",
    "vocab_size=10000\n",
    "embedding_dim=16\n",
    "max_length=120\n",
    "trunc_type='post'\n",
    "oov_tok=\"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, in the word index dictionary, the words are keys and the word tokens are the values. We are going to reverse that dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? b'this is the kind of film for a snowy sunday afternoon when the rest of the world can go ahead with its own business as you <OOV> into a big arm chair and <OOV> for a couple of hours wonderful performances from cher and nicolas cage as always gently row the plot along there are no <OOV> to cross no dangerous waters just a warm and witty <OOV> through new york life at its best a family film in every sense and one that deserves the praise it received '\n",
      "b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded[3]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(6, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4886 - accuracy: 0.7450 - val_loss: 0.3883 - val_accuracy: 0.8238\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2401 - accuracy: 0.9070 - val_loss: 0.4132 - val_accuracy: 0.8218\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0965 - accuracy: 0.9750 - val_loss: 0.5500 - val_accuracy: 0.7966\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0234 - accuracy: 0.9970 - val_loss: 0.6128 - val_accuracy: 0.8066\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0051 - accuracy: 0.9998 - val_loss: 0.7080 - val_accuracy: 0.8036\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7525 - val_accuracy: 0.8083\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.2574e-04 - accuracy: 1.0000 - val_loss: 0.8047 - val_accuracy: 0.8082\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.6181e-04 - accuracy: 1.0000 - val_loss: 0.8535 - val_accuracy: 0.8080\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6465e-04 - accuracy: 1.0000 - val_loss: 0.8977 - val_accuracy: 0.8089\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.6765e-04 - accuracy: 1.0000 - val_loss: 0.9459 - val_accuracy: 0.8074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13344f150>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download('Data/vecs.tsv')\n",
    "  files.download('Data/meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Dataset Type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how the subwords tokenizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenized string is [62, 1893, 605, 3119, 5469, 7997, 2432, 5926, 1916, 987, 90, 69, 9, 48, 4, 6307, 2327, 4043, 4265, 999, 840, 2359, 7975]\n",
      "\n",
      "The original string is 'This notebook contains IMDB datasets! It is from a TensorFlow tutorial.'\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'This notebook contains IMDB datasets! It is from a TensorFlow tutorial.'\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "\n",
    "print(\"The tokenized string is {}\".format(tokenized_string))\n",
    "print(\"\\nThe original string is '{}'\".format(tokenizer.decode(tokenized_string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 ----> This \n",
      "1893 ----> note\n",
      "605 ----> book \n",
      "3119 ----> contains \n",
      "5469 ----> IM\n",
      "7997 ----> D\n",
      "2432 ----> B \n",
      "5926 ----> dat\n",
      "1916 ----> ase\n",
      "987 ----> ts\n",
      "90 ----> ! \n",
      "69 ----> It \n",
      "9 ----> is \n",
      "48 ----> from \n",
      "4 ----> a \n",
      "6307 ----> Ten\n",
      "2327 ----> sor\n",
      "4043 ----> Fl\n",
      "4265 ----> ow \n",
      "999 ----> tu\n",
      "840 ----> tor\n",
      "2359 ----> ial\n",
      "7975 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's build a Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 64)          523840    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 524,237\n",
      "Trainable params: 524,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 10s 25ms/step - loss: 0.0733 - accuracy: 0.9800 - val_loss: 0.5244 - val_accuracy: 0.8581\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 10s 25ms/step - loss: 0.0723 - accuracy: 0.9792 - val_loss: 0.5478 - val_accuracy: 0.8539\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.0672 - accuracy: 0.9827 - val_loss: 0.5527 - val_accuracy: 0.8586\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 9s 23ms/step - loss: 0.0645 - accuracy: 0.9839 - val_loss: 0.5673 - val_accuracy: 0.8575\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 9s 24ms/step - loss: 0.0638 - accuracy: 0.9827 - val_loss: 0.6055 - val_accuracy: 0.8523\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.0595 - accuracy: 0.9849 - val_loss: 0.6040 - val_accuracy: 0.8517\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.0566 - accuracy: 0.9863 - val_loss: 0.6143 - val_accuracy: 0.8537\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.0531 - accuracy: 0.9874 - val_loss: 0.6335 - val_accuracy: 0.8525\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.0510 - accuracy: 0.9883 - val_loss: 0.6496 - val_accuracy: 0.8531\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.0501 - accuracy: 0.9886 - val_loss: 0.6766 - val_accuracy: 0.8474\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
